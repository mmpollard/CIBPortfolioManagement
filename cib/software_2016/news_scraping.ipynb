{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymongo\n",
    "import MySQLdb as sql\n",
    "import _mysql\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt; import matplotlib.pylab as pylab\n",
    "#%matplotlib inline\n",
    "pd.options.display.mpl_style = 'default'\n",
    "pylab.rcParams['figure.figsize'] = 12, 6\n",
    "from dateutil import parser\n",
    "import Quandl\n",
    "from pymongo import MongoClient\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get A BeautifulSoup Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BloombergSearch:\n",
    "    def __init__(self, search_term):\n",
    "        self.search_term = search_term\n",
    "        self.url_page1 = ('http://www.bloomberg.com/search?query=' + str(self.search_term))\n",
    "\n",
    "    def get_search_soup(self):\n",
    "        url =  self.url_page1\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "    def get_soup(self, url):\n",
    "        page = urllib2.urlopen(url).read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        return soup\n",
    "    def get_search_page_links(self, num_pages):\n",
    "        article_list = []\n",
    "        for i in range(1, num_pages + 1):\n",
    "            temp_soup = self.get_soup(self.url_page1 + str('&page=') + str(i))\n",
    "            for result in temp_soup.find_all('h1'):\n",
    "                try:\n",
    "                    if 'video' in result.a['href']:\n",
    "                        continue\n",
    "                    if 'http' in result.a['href']:\n",
    "                        #print item.a['href']\n",
    "                        article_list.append(result.a['href'])\n",
    "                    else:\n",
    "                        #print 'http://www.bloomberg.com/' + item.a['href']\n",
    "                        article_list.append('http://www.bloomberg.com/' + result.a['href'])\n",
    "                except:\n",
    "                    continue\n",
    "            #print 'Added page=' + str(i)\n",
    "        return article_list\n",
    "    \n",
    "    def get_post_body(self, article_soup):\n",
    "        final_text = \"\"\n",
    "        query = article_soup.find_all('div',  class_=\"article-body__content\")\n",
    "        for item in query:\n",
    "            for text in item.find_all('p'):\n",
    "                final_text = final_text + '\\n\\n' + str(text.text.encode('utf-8'))\n",
    "        if final_text == \"\":\n",
    "            return None\n",
    "        return final_text\n",
    "    \n",
    "    def get_post_date(self, article_soup):\n",
    "        final_text = \"\"\n",
    "        result = article_soup.find('time', class_ = \"published-at\")\n",
    "        try:\n",
    "            return result['datetime']\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_post_author(self, article_soup):\n",
    "        final_text = \"\"\n",
    "        result = article_soup.find('a', class_ = \"author-link\")\n",
    "        try:\n",
    "            return result.text.lstrip().rstrip()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_post_title(self, article_soup):\n",
    "        final_text = \"\"\n",
    "        result = article_soup.find('title')\n",
    "        try:\n",
    "            return result.text.lstrip().rstrip()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def make_info(self, pages = 1):\n",
    "        final_df = pd.DataFrame()\n",
    "        for url in self.get_search_page_links(pages):\n",
    "            temp_soup = self.get_soup(url)\n",
    "            body = self.get_post_body(temp_soup)\n",
    "            title = self.get_post_title(temp_soup)\n",
    "            author = self.get_post_author(temp_soup)\n",
    "            date = self.get_post_date(temp_soup)\n",
    "            temp_series = pd.Series([title, url, author, date, body])\n",
    "            final_df= final_df.append(temp_series, ignore_index = True)\n",
    "            #print 'Added article, ' + str(len(final_df))\n",
    "        final_df.columns = ['title', 'url', 'author', 'date', 'text']\n",
    "        return final_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReutersSearch:\n",
    "    def __init__(self, search_term):\n",
    "        self.search_term = search_term\n",
    "        self.url_page1 = ('http://www.reuters.com/search/news?blob=' + str(self.search_term))\n",
    "\n",
    "    def get_search_soup(self):\n",
    "        url =  self.url_page1\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "    def get_soup(self, url):\n",
    "        page = urllib2.urlopen(url).read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        return soup\n",
    "    def get_search_page_links(self, num_pages = 1):\n",
    "        #Does not have a next page link, only a\n",
    "        #'LOAD MORE RESULTS' element\n",
    "        article_list = []\n",
    "        temp_page_url = self.url_page1\n",
    "        temp_soup = self.get_soup(temp_page_url)\n",
    "        query = temp_soup.find_all('h3', class_ = 'search-result-title')\n",
    "        for result in query:\n",
    "            try:\n",
    "                if 'video' in result.a['href']:\n",
    "                    print 'video on ' + str(i)\n",
    "                    continue\n",
    "                if 'http' in result.a['href']:\n",
    "                    article_list.append(result.a['href'])\n",
    "                else:\n",
    "                    article_list.append('http://www.reuters.com/' + result.a['href'])\n",
    "            except:\n",
    "                continue\n",
    "        return article_list\n",
    "    \n",
    "    def get_post_body(self, article_soup):\n",
    "        final_text = \"\"\n",
    "        query = article_soup.find_all('div',  id_ = \"storytext\")\n",
    "        for item in query:\n",
    "            for text in item.find_all('p'):\n",
    "                final_text = final_text + '\\n\\n' + str(text.text.encode('utf-8'))\n",
    "        if final_text == \"\":\n",
    "            return None\n",
    "        return final_text\n",
    "    \n",
    "    def get_post_date(self, article_soup):\n",
    "        final_text = \"\"\n",
    "        result = article_soup.find('span', class_=\"cnnDateStamp\")\n",
    "        try:\n",
    "            return result.text\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_post_author(self, article_soup):\n",
    "        final_text = \"\"\n",
    "        result = article_soup.find('span', class_ = 'byline')\n",
    "        try:\n",
    "            return result.text.split('by')[1].lstrip().rstrip()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_post_title(self, article_soup):\n",
    "        final_text = \"\"\n",
    "        result = article_soup.find('h1', class_ = 'article_title')\n",
    "        try:\n",
    "            return result.text.lstrip().rstrip()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def make_info(self, pages = 1):\n",
    "        final_df = pd.DataFrame()\n",
    "        for url in self.get_search_page_links(pages):\n",
    "            temp_soup = self.get_soup(url)\n",
    "            body = self.get_post_body(temp_soup)\n",
    "            title = self.get_post_title(temp_soup)\n",
    "            author = self.get_post_author(temp_soup)\n",
    "            date = self.get_post_date(temp_soup)\n",
    "            temp_series = pd.Series([title, url, author, date, body])\n",
    "            final_df= final_df.append(temp_series, ignore_index = True)\n",
    "            #print 'Added article, ' + str(len(final_df))\n",
    "        final_df.columns = ['title', 'url', 'author', 'date', 'text']\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN = ReutersSearch('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp = CNN.get_search_page_links(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://blogs.reuters.com/alison-frankel/2016/02/23/in-unsealed-filing-doj-claims-apple-misleading-on-decryption-opposition/',\n",
       " 'http://www.reuters.com/article/idUSL2N1630LS',\n",
       " 'http://www.reuters.com/article/idUSL2N163090',\n",
       " 'http://www.reuters.com/article/idUSKCN0VX159',\n",
       " 'http://www.reuters.com/article/idUSL2N1622IH',\n",
       " 'http://www.reuters.com/article/idUSL8N16252Z',\n",
       " 'http://www.reuters.com/article/idUSKCN0VW0BM',\n",
       " 'http://www.reuters.com/article/idUSL3N162580',\n",
       " 'http://www.reuters.com/article/idUSL2N1620IJ',\n",
       " 'http://www.reuters.com/article/idUSL3N1613U4']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
